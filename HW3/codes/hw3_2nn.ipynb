{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPEoabX-hGCh"
   },
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CfcNKryaj6VJ",
    "ExecuteTime": {
     "end_time": "2023-11-16T13:12:50.674713Z",
     "start_time": "2023-11-16T13:12:50.540974Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n6CbjnM5kbBO",
    "ExecuteTime": {
     "end_time": "2023-11-16T13:12:50.675194Z",
     "start_time": "2023-11-16T13:12:50.543442Z"
    }
   },
   "outputs": [],
   "source": [
    "# %cd '/content/drive/PATH/TO/MNIST_DATA_FILE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OyammZP8hI7P",
    "ExecuteTime": {
     "end_time": "2023-11-16T13:12:51.448687Z",
     "start_time": "2023-11-16T13:12:50.548534Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from mnist.data_utils import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLxTNOvI5NHD"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xuQB6W2U5ZE2",
    "ExecuteTime": {
     "end_time": "2023-11-16T13:21:50.609442Z",
     "start_time": "2023-11-16T13:21:50.597156Z"
    }
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Implement the elu activation function.\n",
    "    The method takes the input z and returns the output of the function.\n",
    "    Please DO NOT MODIFY the alpha value.\n",
    "\n",
    "    Question (a)\n",
    "\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE #####\n",
    "    return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
    "    #####################\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    \"\"\"\n",
    "    Implement the softmax function.\n",
    "    The method takes the input X and returns the output of the function.\n",
    "\n",
    "    Question (a)\n",
    "\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE #####\n",
    "    e_z = np.exp(X - np.max(X, axis=1, keepdims=True))  # Subtracting np.max(z) for numerical stability\n",
    "    return e_z / e_z.sum(axis=1, keepdims=True)\n",
    "    #####################\n",
    "\n",
    "def deriv_elu(x, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Implement the derivative of elu activation function.\n",
    "    The method takes the input z and returns the output of the function.\n",
    "    Please DO NOT MODIFY the alpha value.\n",
    "\n",
    "    Question (a)\n",
    "\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE #####\n",
    "    elu_val = np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "    return np.where(x > 0, 1, elu_val + alpha)\n",
    "    #####################\n",
    "\n",
    "def load_batch(X, Y, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates batches with the remainder dropped.\n",
    "\n",
    "    Do NOT modify this function\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        permutation = np.random.permutation(X.shape[0])\n",
    "        X = X[permutation, :]\n",
    "        Y = Y[permutation, :]\n",
    "    num_steps = int(X.shape[0])//batch_size\n",
    "    step = 0\n",
    "    while step<num_steps:\n",
    "        X_batch = X[batch_size*step:batch_size*(step+1)]\n",
    "        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n",
    "        step+=1\n",
    "        yield X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsU8v_6khR30"
   },
   "source": [
    "# 2-Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mA5udiGmhRb5",
    "ExecuteTime": {
     "end_time": "2023-11-16T13:21:52.971769Z",
     "start_time": "2023-11-16T13:21:52.950824Z"
    }
   },
   "outputs": [],
   "source": [
    "class TwoLayerNN:\n",
    "    \"\"\" a neural network with 2 layers \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_hiddens, num_classes):\n",
    "        \"\"\"\n",
    "        Do NOT modify this function.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_classes = num_classes\n",
    "        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n",
    "\n",
    "    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n",
    "        \"\"\"\n",
    "        initializes parameters with Xavier Initialization.\n",
    "\n",
    "        Question (b)\n",
    "        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization\n",
    "\n",
    "        Inputs\n",
    "        - input_dim\n",
    "        - num_hiddens\n",
    "        - num_classes\n",
    "        Returns\n",
    "        - params: a dictionary with the initialized parameters.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        ##### YOUR CODE #####\n",
    "        # Xavier initialization for weights\n",
    "        limit_1 = np.sqrt(6 / float(input_dim + num_hiddens))\n",
    "        params['W1'] = np.random.uniform(low=-limit_1, high=limit_1, size=(input_dim, num_hiddens))\n",
    "        params['b1'] = np.zeros((1, num_hiddens))\n",
    "        \n",
    "        limit_2 = np.sqrt(6 / float(num_hiddens + num_classes))\n",
    "        params['W2'] = np.random.uniform(low=-limit_2, high=limit_2, size=(num_hiddens, num_classes))\n",
    "        params['b2'] = np.zeros((1, num_classes))\n",
    "\n",
    "        #####################\n",
    "        return params\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Define and perform the feed forward step of a two-layer neural network.\n",
    "        Specifically, the network structure is given by\n",
    "\n",
    "          y = softmax(ELU(X W1 + b1) W2 + b2)\n",
    "\n",
    "        where X is the input matrix of shape (N, D), y is the class distribution matrix\n",
    "        of shape (N, C), N is the number of examples (either the entire dataset or\n",
    "        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n",
    "\n",
    "        Question (c)\n",
    "        - ff_dict will be used to run backpropagation in backward method.\n",
    "\n",
    "        Inputs\n",
    "        - X: the input matrix of shape (N, D)\n",
    "\n",
    "        Returns\n",
    "        - y: the output of the model\n",
    "        - ff_dict: a dictionary with all the fully connected units and activations.\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE #####\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "\n",
    "        # First fully connected layer\n",
    "        fc1 = np.dot(X, W1) + b1\n",
    "\n",
    "        # ELU activation\n",
    "        elu_out = elu(fc1)\n",
    "\n",
    "        # Second fully connected layer\n",
    "        fc2 = np.dot(elu_out, W2) + b2\n",
    "\n",
    "        # Softmax output\n",
    "        y = softmax(fc2)\n",
    "\n",
    "        # Storing the results in a dictionary\n",
    "        ff_dict = {\n",
    "            'fc1': fc1,\n",
    "            'elu_out': elu_out,\n",
    "            'fc2': fc2,\n",
    "            'y': y\n",
    "        }\n",
    "        #####################\n",
    "        return y, ff_dict\n",
    "\n",
    "    def backward(self, X, Y, ff_dict):\n",
    "        \"\"\"\n",
    "        Performs backpropagation over the two-layer neural network, and returns\n",
    "        a dictionary of gradients of all model parameters.\n",
    "\n",
    "        Question (d)\n",
    "\n",
    "        Inputs:\n",
    "         - X: the input matrix of shape (B, D), where B is the number of examples\n",
    "              in a mini-batch, D is the feature dimensionality.\n",
    "         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n",
    "              where B is the number of examples in a mini-batch, C is the number\n",
    "              of classes.\n",
    "         - ff_dict: the dictionary containing all the fully connected units and\n",
    "              activations.\n",
    "\n",
    "        Returns:\n",
    "         - grads: a dictionary containing the gradients of corresponding weights and biases.\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE #####\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "\n",
    "        # Number of samples\n",
    "        num_samples = X.shape[0]\n",
    "    \n",
    "        # Gradients dictionary\n",
    "        grads = {}\n",
    "    \n",
    "        # Output error\n",
    "        y_out = ff_dict['y']\n",
    "        dY = y_out - Y  # Gradient of cross-entropy loss with softmax\n",
    "\n",
    "        # Backprop through second fully connected layer\n",
    "        dW2 = np.dot(ff_dict['elu_out'].T, dY) / num_samples\n",
    "        db2 = np.sum(dY, axis=0, keepdims=True) / num_samples \n",
    "\n",
    "        # Backprop through ELU activation\n",
    "        d_elu = np.dot(dY, W2.T) * deriv_elu(ff_dict['fc1'])\n",
    "\n",
    "        # Backprop through first fully connected layer\n",
    "        dW1 = np.dot(X.T, d_elu) / num_samples\n",
    "        db1 = np.sum(d_elu, axis=0, keepdims=True) / num_samples\n",
    "\n",
    "        # Store the gradients\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "    \n",
    "        #####################\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Computes cross entropy loss.\n",
    "\n",
    "        Do NOT modify this function.\n",
    "\n",
    "        Inputs\n",
    "            Y:\n",
    "            Y_hat:\n",
    "        Returns\n",
    "            loss:\n",
    "        \"\"\"\n",
    "        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "        return loss\n",
    "\n",
    "    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n",
    "        \"\"\"\n",
    "        Runs mini-batch gradient descent.\n",
    "\n",
    "        Do NOT Modify this method.\n",
    "\n",
    "        Inputs\n",
    "        - X\n",
    "        - Y\n",
    "        - X_val\n",
    "        - Y_Val\n",
    "        - lr\n",
    "        - n_epochs\n",
    "        - batch_size\n",
    "        - log_interval\n",
    "        \"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "          for X_batch, Y_batch in load_batch(X, Y, batch_size):\n",
    "              self.train_step(X_batch, Y_batch, batch_size, lr)\n",
    "          if epoch % log_interval==0:\n",
    "              Y_hat, ff_dict = self.forward(X)\n",
    "              train_loss = self.compute_loss(Y, Y_hat)\n",
    "              train_acc = self.evaluate(Y, Y_hat)\n",
    "              Y_hat, ff_dict = self.forward(X_val)\n",
    "              valid_loss = self.compute_loss(Y_val, Y_hat)\n",
    "              valid_acc = self.evaluate(Y_val, Y_hat)\n",
    "              print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n",
    "                    format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "\n",
    "    def train_step(self, X_batch, Y_batch, batch_size, lr):\n",
    "        \"\"\"\n",
    "        Updates the parameters using gradient descent.\n",
    "\n",
    "        Do NOT Modify this method.\n",
    "\n",
    "        Inputs\n",
    "        - X_batch\n",
    "        - Y_batch\n",
    "        - batch_size\n",
    "        - lr\n",
    "        \"\"\"\n",
    "        _, ff_dict = self.forward(X_batch)\n",
    "        grads = self.backward(X_batch, Y_batch, ff_dict)\n",
    "        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n",
    "        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n",
    "        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n",
    "        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n",
    "\n",
    "    def evaluate(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Computes classification accuracy.\n",
    "\n",
    "        Do NOT modify this function\n",
    "\n",
    "        Inputs\n",
    "        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n",
    "             where C is the number of classes.\n",
    "        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n",
    "             where C is the number of classes.\n",
    "\n",
    "        Returns\n",
    "            accuracy: the classification accuracy in float\n",
    "        \"\"\"\n",
    "        classes_pred = np.argmax(Y_hat, axis=1)\n",
    "        classes_gt = np.argmax(Y, axis=1)\n",
    "        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXM2lWhtDYC6"
   },
   "source": [
    "# Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "48ooR6YIxYhC",
    "ExecuteTime": {
     "end_time": "2023-11-16T13:12:52.143861Z",
     "start_time": "2023-11-16T13:12:51.470584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST data loaded:\n",
      "Training data shape: (60000, 784)\n",
      "Training labels shape: (60000, 10)\n",
      "Test data shape: (10000, 784)\n",
      "Test labels shape: (10000, 10)\n",
      "\n",
      "Set validation data aside\n",
      "Training data shape:  (48000, 784)\n",
      "Training labels shape:  (48000, 10)\n",
      "Validation data shape:  (12000, 784)\n",
      "Validation labels shape:  (12000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_data()\n",
    "\n",
    "idxs = np.arange(len(X_train))\n",
    "np.random.shuffle(idxs)\n",
    "split_idx = int(np.ceil(len(idxs)*0.8))\n",
    "X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n",
    "X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n",
    "print()\n",
    "print('Set validation data aside')\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', Y_train.shape)\n",
    "print('Validation data shape: ', X_valid.shape)\n",
    "print('Validation labels shape: ', Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzw-D4Zr5xoi"
   },
   "source": [
    "# Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IlnC_rerHPaN",
    "ExecuteTime": {
     "end_time": "2023-11-16T13:12:52.144652Z",
     "start_time": "2023-11-16T13:12:52.142981Z"
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Question (e)\n",
    "# Tune the hyperparameters with validation data,\n",
    "# and print the results by running the lines below.\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TTCqVT4S0Tm5",
    "ExecuteTime": {
     "end_time": "2023-11-16T13:22:10.187808Z",
     "start_time": "2023-11-16T13:22:10.179941Z"
    }
   },
   "outputs": [],
   "source": [
    "# model instantiation\n",
    "model = TwoLayerNN(input_dim=784, num_hiddens=128, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6cWb6xg0NxOs",
    "ExecuteTime": {
     "end_time": "2023-11-17T10:50:50.934995Z",
     "start_time": "2023-11-17T10:46:52.658041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00 - train loss/acc: 0.116 0.968, valid loss/acc: 0.152 0.956\n",
      "epoch 01 - train loss/acc: 0.113 0.969, valid loss/acc: 0.151 0.956\n",
      "epoch 02 - train loss/acc: 0.107 0.971, valid loss/acc: 0.145 0.958\n",
      "epoch 03 - train loss/acc: 0.102 0.972, valid loss/acc: 0.140 0.958\n",
      "epoch 04 - train loss/acc: 0.098 0.973, valid loss/acc: 0.138 0.960\n",
      "epoch 05 - train loss/acc: 0.095 0.974, valid loss/acc: 0.135 0.960\n",
      "epoch 06 - train loss/acc: 0.091 0.975, valid loss/acc: 0.131 0.961\n",
      "epoch 07 - train loss/acc: 0.088 0.976, valid loss/acc: 0.129 0.962\n",
      "epoch 08 - train loss/acc: 0.086 0.976, valid loss/acc: 0.127 0.963\n",
      "epoch 09 - train loss/acc: 0.084 0.977, valid loss/acc: 0.125 0.963\n",
      "epoch 10 - train loss/acc: 0.081 0.978, valid loss/acc: 0.123 0.963\n",
      "epoch 11 - train loss/acc: 0.078 0.979, valid loss/acc: 0.121 0.964\n",
      "epoch 12 - train loss/acc: 0.076 0.979, valid loss/acc: 0.119 0.966\n",
      "epoch 13 - train loss/acc: 0.074 0.980, valid loss/acc: 0.117 0.967\n",
      "epoch 14 - train loss/acc: 0.071 0.981, valid loss/acc: 0.116 0.966\n",
      "epoch 15 - train loss/acc: 0.069 0.981, valid loss/acc: 0.115 0.967\n",
      "epoch 16 - train loss/acc: 0.067 0.982, valid loss/acc: 0.113 0.967\n",
      "epoch 17 - train loss/acc: 0.065 0.982, valid loss/acc: 0.112 0.968\n",
      "epoch 18 - train loss/acc: 0.064 0.983, valid loss/acc: 0.111 0.968\n",
      "epoch 19 - train loss/acc: 0.062 0.984, valid loss/acc: 0.109 0.969\n",
      "epoch 20 - train loss/acc: 0.060 0.984, valid loss/acc: 0.109 0.969\n",
      "epoch 21 - train loss/acc: 0.059 0.984, valid loss/acc: 0.107 0.970\n",
      "epoch 22 - train loss/acc: 0.057 0.985, valid loss/acc: 0.107 0.970\n",
      "epoch 23 - train loss/acc: 0.056 0.985, valid loss/acc: 0.106 0.971\n",
      "epoch 24 - train loss/acc: 0.056 0.984, valid loss/acc: 0.106 0.970\n",
      "epoch 25 - train loss/acc: 0.054 0.986, valid loss/acc: 0.104 0.971\n",
      "epoch 26 - train loss/acc: 0.052 0.986, valid loss/acc: 0.103 0.971\n",
      "epoch 27 - train loss/acc: 0.050 0.987, valid loss/acc: 0.102 0.971\n",
      "epoch 28 - train loss/acc: 0.050 0.987, valid loss/acc: 0.101 0.972\n",
      "epoch 29 - train loss/acc: 0.048 0.987, valid loss/acc: 0.100 0.972\n",
      "epoch 30 - train loss/acc: 0.048 0.987, valid loss/acc: 0.101 0.972\n",
      "epoch 31 - train loss/acc: 0.046 0.988, valid loss/acc: 0.099 0.972\n",
      "epoch 32 - train loss/acc: 0.045 0.988, valid loss/acc: 0.100 0.972\n",
      "epoch 33 - train loss/acc: 0.044 0.988, valid loss/acc: 0.099 0.972\n",
      "epoch 34 - train loss/acc: 0.043 0.988, valid loss/acc: 0.099 0.972\n",
      "epoch 35 - train loss/acc: 0.042 0.989, valid loss/acc: 0.099 0.972\n",
      "epoch 36 - train loss/acc: 0.042 0.989, valid loss/acc: 0.097 0.973\n",
      "epoch 37 - train loss/acc: 0.040 0.990, valid loss/acc: 0.097 0.972\n",
      "epoch 38 - train loss/acc: 0.040 0.990, valid loss/acc: 0.098 0.972\n",
      "epoch 39 - train loss/acc: 0.039 0.990, valid loss/acc: 0.096 0.973\n",
      "epoch 40 - train loss/acc: 0.038 0.990, valid loss/acc: 0.097 0.973\n",
      "epoch 41 - train loss/acc: 0.037 0.991, valid loss/acc: 0.096 0.973\n",
      "epoch 42 - train loss/acc: 0.036 0.991, valid loss/acc: 0.095 0.973\n",
      "epoch 43 - train loss/acc: 0.036 0.991, valid loss/acc: 0.096 0.974\n",
      "epoch 44 - train loss/acc: 0.035 0.992, valid loss/acc: 0.095 0.974\n",
      "epoch 45 - train loss/acc: 0.034 0.992, valid loss/acc: 0.095 0.974\n",
      "epoch 46 - train loss/acc: 0.033 0.992, valid loss/acc: 0.095 0.973\n",
      "epoch 47 - train loss/acc: 0.033 0.992, valid loss/acc: 0.096 0.972\n",
      "epoch 48 - train loss/acc: 0.032 0.993, valid loss/acc: 0.094 0.974\n",
      "epoch 49 - train loss/acc: 0.032 0.992, valid loss/acc: 0.094 0.974\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "lr, n_epochs, batch_size = 0.5, 50, 32\n",
    "model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hpPsAlXU0T_Z",
    "ExecuteTime": {
     "end_time": "2023-11-17T10:50:51.041574Z",
     "start_time": "2023-11-17T10:50:50.957469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test loss = 0.081, acc = 0.975\n"
     ]
    }
   ],
   "source": [
    "# evalute the model on test data\n",
    "Y_hat, _ = model.forward(X_test)\n",
    "test_loss = model.compute_loss(Y_test, Y_hat)\n",
    "test_acc = model.evaluate(Y_test, Y_hat)\n",
    "print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Question (e) attempts:\n",
    "\n",
    "lr, n_epochs, batch_size = 0.1, 50, 256\n",
    "Final test loss = 0.383, acc = 0.895\n",
    "\n",
    "lr, n_epochs, batch_size = 0.3, 50, 256\n",
    "Final test loss = 0.417, acc = 0.888\n",
    "\n",
    "lr, n_epochs, batch_size = 0.1, 50, 128\n",
    "Final test loss = 0.324, acc = 0.909\n",
    "\n",
    "lr, n_epochs, batch_size = 0.3, 50, 128\n",
    "Final test loss = 0.280, acc = 0.921\n",
    "\n",
    "lr, n_epochs, batch_size = 0.5, 50, 128\n",
    "Final test loss = 0.239, acc = 0.933\n",
    "\n",
    "lr, n_epochs, batch_size = 0.5, 50, 64\n",
    "Final test loss = 0.134, acc = 0.962.\n",
    "\n",
    "lr, n_epochs, batch_size = 0.5, 50, 32\n",
    "Final test loss = 0.081, acc = 0.975     # Best result\n",
    "\n",
    "lr, n_epochs, batch_size = 0.01, 100, 256\n",
    "Final test loss = 0.378, acc = 0.896"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
