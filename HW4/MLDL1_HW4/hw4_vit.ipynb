{"cells":[{"cell_type":"markdown","id":"mDOdbok4zVE4","metadata":{"id":"mDOdbok4zVE4"},"source":["# Colab Setup"]},{"cell_type":"code","execution_count":null,"id":"-NvjxSCiMfMq","metadata":{"id":"-NvjxSCiMfMq"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"oS5qDEF5MftN","metadata":{"id":"oS5qDEF5MftN"},"outputs":[],"source":["\"\"\"\n","Change directory to where this file is located\n","\"\"\"\n","%cd 'COPY&PASTE FILE DIRECTORY HERE'"]},{"cell_type":"markdown","id":"giBbJLW-zhS-","metadata":{"id":"giBbJLW-zhS-"},"source":["# Import Modules"]},{"cell_type":"code","execution_count":null,"id":"KjGAWailzgP1","metadata":{"id":"KjGAWailzgP1"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","from torchvision import transforms, datasets\n","\n","from tqdm.auto import tqdm"]},{"cell_type":"markdown","id":"2f33b210","metadata":{"id":"2f33b210"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"id":"-ANUeGuyNGua","metadata":{"id":"-ANUeGuyNGua"},"outputs":[],"source":["def train(model, train_loader, optimizer, criterion, DEVICE):\n","    \"\"\"\n","    Trains the model with training data.\n","\n","    Do NOT modify this function.\n","    \"\"\"\n","    model.train()\n","    tqdm_bar = tqdm(train_loader)\n","    for batch_idx, (image, label) in enumerate(tqdm_bar):\n","        image = image.to(DEVICE)\n","        label = label.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(image)\n","        loss = criterion(output, label)\n","        loss.backward()\n","        optimizer.step()\n","        tqdm_bar.set_description(\"Epoch {} - train loss: {:.6f}\".format(epoch, loss.item()))\n","\n","\n","def evaluate(model, test_loader, criterion, DEVICE):\n","    \"\"\"\n","    Evaluates the trained model with test data.\n","\n","    Do NOT modify this function.\n","    \"\"\"\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for image, label in tqdm(test_loader):\n","            image = image.to(DEVICE)\n","            label = label.to(DEVICE)\n","            output = model(image)\n","            test_loss += criterion(output, label).item()\n","            prediction = output.max(1, keepdim=True)[1]\n","            correct += prediction.eq(label.view_as(prediction)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100. * correct / len(test_loader.dataset)\n","    return test_loss, test_accuracy"]},{"cell_type":"markdown","id":"2eeea83e","metadata":{"id":"2eeea83e"},"source":["# ViT Model"]},{"cell_type":"code","execution_count":null,"id":"P0xUGjocL4OH","metadata":{"id":"P0xUGjocL4OH"},"outputs":[],"source":["###\n","# Always check tensor shapes!\n","# Printing shapes can be the fastest way to track the error and fix it.\n","# Using dropout is optional. You don't have to use pre-declared dropouts.\n","###"]},{"cell_type":"code","source":["class Patchification(nn.Module):\n","  \"\"\"\n","  Question (a)\n","  Process the batch of images to non-overlapping patches using convolution layer.\n","  You are only allowed to use torch.nn.Con2d\n","\n","  - Input shape: [batch, channel, height, width]\n","  - Return: [batch, number_of_patches, embedding_dimension]\n","  \"\"\"\n","  def __init__(self, in_channels, patch_size, embedding_dim):\n","    super().__init__()\n","    \"\"\"\n","    Hint: embedding_dim should be the out_channel of convolution.\n","    \"\"\"\n","    ##### YOUR CODE #####\n","\n","\n","    #####################\n","\n","  def forward(self, x):\n","    ##### YOUR CODE #####\n","\n","\n","    #####################\n","    return x"],"metadata":{"id":"ZlLE3KG6VN6h"},"id":"ZlLE3KG6VN6h","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Hint cell.\n","You can briefly check if the function is implemented correctly by printing the shape of the output before jumping into the training with GPU.\n","Example code is written in the below.\n","\"\"\"\n","batch, channel, height, width = 5, 3, 32, 32\n","patch_size = (4, 4)\n","embedding_dim = 128\n","\n","test_img = torch.ones(batch, channel, height, width)\n","\n","patchification = Patchification(channel, patch_size, embedding_dim)\n","after_patchification = patchification(test_img)\n","\n","print(after_patchification.shape) # Is the shape [batch, number_of_patches, dim] ?"],"metadata":{"id":"MxHDtbXaOEpy"},"id":"MxHDtbXaOEpy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MLP(nn.Module):\n","  \"\"\"\n","  Feed-forward layer\n","\n","  - Input shape: [batch, number_of_patches, embedding_dimension]\n","  - Return: [batch, number_of_patches, embedding_dimension]\n","\n","  Do NOT modify.\n","  \"\"\"\n","  def __init__(self, dim, hidden_dim, dropout = 0.):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        x = self.layer(x)\n","        return x"],"metadata":{"id":"0qXGdfdPVRGq"},"id":"0qXGdfdPVRGq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Attention(nn.Module):\n","  \"\"\"\n","  Question (b)\n","\n","  Implement Multi-head attention class.\n","  Please refer to part 3.2 in the paper \"Attention is All You Need\"\n","  You can implement without considering heads (i.e. vanilla attention).\n","  However, your maximum score will be 5 points.\n","\n","  - Input shape: [batch, number_of_patches, embedding_dimension]\n","  - Return: [batch, number_of_patches, embedding_dimension]\n","  \"\"\"\n","  def __init__(self, dim, num_heads, dropout = 0.):\n","    super().__init__()\n","    \"\"\"\n","    Do NOT modify.\n","    \"\"\"\n","    self.head_dim = dim // num_heads\n","    self.dim = dim\n","    self.num_heads = num_heads\n","\n","    self.scale = self.head_dim ** 0.5 # Don't forget scaling!\n","    # If you are not going to consider head, you need to change self.scale as below.\n","    # self.scale = self.dim ** 0.5\n","\n","    self.dropout = nn.Dropout(dropout)\n","\n","    ##### YOUR CODE #####\n","    \"\"\"\n","    You need to define some layers...\n","    \"\"\"\n","\n","\n","\n","    #####################\n","\n","  def forward(self, x):\n","    ##### YOUR CODE #####\n","\n","\n","    #####################\n","    return x\n","\n","class Block(nn.Module):\n","  \"\"\"\n","  Question (c)\n","  Complete Block class.\n","\n","  - Input shape: [batch, number_of_patches, embedding_dimension]\n","  - Return: [batch, number_of_patches, embedding_dimension]\n","  \"\"\"\n","  def __init__(self, dim, num_heads, mlp_dim, dropout=0.):\n","    super().__init__()\n","    ##### YOUR CODE #####\n","\n","\n","    #####################\n","\n","  def forward(self, x):\n","    \"\"\"\n","    Hint: Don't forget the residual connections!\n","    \"\"\"\n","    ##### YOUR CODE #####\n","\n","\n","    #####################\n","    return x"],"metadata":{"id":"W6pA1fMvVSv5"},"id":"W6pA1fMvVSv5","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"c4d35830","metadata":{"id":"c4d35830"},"outputs":[],"source":["class ViT(nn.Module):\n","    def __init__(self, image_shape, patch_size, num_classes, dim, num_heads, depth, mlp_dim, dropout = 0.):\n","        super().__init__()\n","        \"\"\"\n","        image_shape: [channel, height, width]\n","        patch_size: [height, width]\n","        dim: Embedding dimension\n","        num_heads: Number of heads to be used in Multi-head Attention\n","        depth: Number of attention blocks to be used\n","        mlp_dim: Hidden dimension to be used in MLP layer (=feedforward layer)\n","        \"\"\"\n","\n","        image_ch, image_h, image_w = image_shape # image_ch will be 3(RGB 3 channels) for CIFAR10 dataset\n","        patch_h, patch_w = patch_size\n","\n","        assert image_h % patch_h == 0 and image_w % patch_w == 0, 'Image height & width must be divisible by those of patch respectively.'\n","        assert dim % num_heads == 0, 'Embedding dimension should be divisible by number of heads.'\n","        num_patches = (image_h // patch_h) * (image_w // patch_w) # e.g. [32 x 32] image & [8 x 8] patch size -> [4 x 4 = 16] patches\n","\n","        ##### YOUR CODE #####\n","        \"\"\"Define Patchification using convolution.\n","        \"\"\"\n","        self.patchify =\n","\n","        \"\"\"Define Learnable positional encoding, 1+ is for class token.\n","           Hint: use nn.Parameter\n","        \"\"\"\n","        self.pos_embedding =\n","\n","        \"\"\"Define Class token which will be prepended to each image.\n","           Hint: use nn.Parameter\n","        \"\"\"\n","        self.cls_token =\n","        #####################\n","\n","\n","        # Initialize attention blocks\n","        self.attention_blocks = nn.ModuleList([\n","            Block(dim, num_heads, mlp_dim, dropout)\n","            for _ in range(depth)\n","        ])\n","\n","        # Classification head, maps the final vector to class dimension.\n","        self.classification_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        \"\"\"\n","        Question (d)\n","\n","        ViT forward process\n","\n","        [Hint]\n","        - After patchification, shape will be [batch, number_of_patches, dim].\n","        - If you successfully prepend cls_tokens to this batch of patchfied images, shape will be [batch, 1+ number_of_patches, dim].\n","        - Then simply add the positional embedding.\n","        - Now the tokens(patches) are ready to go through the attention blocks.\n","        - After attention operation, classify with class token. (Simply take off it from whole tokens)\n","        \"\"\"\n","        ##### YOUR CODE #####\n","        cls_tokens =  # Shape: [batch, 1, dim]\n","\n","\n","        #####################\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"6tOh1In3x3FM"},"source":["# ViT Image Classification"],"id":"6tOh1In3x3FM"},{"cell_type":"code","execution_count":null,"id":"VbHlUuyTdhK2","metadata":{"id":"VbHlUuyTdhK2"},"outputs":[],"source":["\"\"\"\n","Make sure your runtime type is GPU and you are using PyTorch version higher than 1.8!\n","\n","Do NOT modify.\n","\"\"\"\n","DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE))"]},{"cell_type":"code","source":["\"\"\"\n","You may change some settings including batch size & augmentations.\n","But if your implementation is correct, default setting is enough to achieve the target performance(i.e. 65%).\n","\"\"\"\n","BATCH_SIZE = 100\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","# Prepare Dataset & DataLoader\n","trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)"],"metadata":{"id":"mOGBSL3bz0jX"},"id":"mOGBSL3bz0jX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Question (e)\n","Train your ViT to achieve 65% of accuarcy.\n","Feel free to change the hyperparameters.\n","But again, if your implementation is correct, default setting is enough to achieve the target performance(i.e. 65%).\n","\"\"\"\n","# Hyperparameters\n","EPOCHS = 10\n","patch_size = (4,4)\n","dim = 128\n","depth = 8\n","num_heads = 8\n","mlp_dim = 256\n","dropout = 0.\n","learning_rate = 0.001\n","\n","model = ViT(image_shape = (3,32,32), patch_size = patch_size, num_classes = 10, dim = dim, num_heads = num_heads, depth = depth, mlp_dim = mlp_dim, dropout=dropout).to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"bzkZ228b8rN5"},"id":"bzkZ228b8rN5","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"sjeFjn0qrPRK","metadata":{"id":"sjeFjn0qrPRK"},"outputs":[],"source":["\"\"\"\n","Do NOT modify.\n","It will take less than 8 minutes for training with default setting.\n","\"\"\"\n","for epoch in range(1, EPOCHS + 1):\n","    train(model, trainloader, optimizer, criterion)\n","    test_loss, test_accuracy = evaluate(model, testloader, criterion)\n","    print(\"\\n[EPOCH: {}], \\tModel: ViT, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n","        epoch, test_loss, test_accuracy))"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}