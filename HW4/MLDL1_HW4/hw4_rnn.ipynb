{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Environment Setup"
   ],
   "metadata": {
    "id": "xvh969t1VJzj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kO4EhYqCTdbt"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Change directory to where this file is located\n",
    "\"\"\"\n",
    "%cd 'COPY&PASTE FILE DIRECTORY HERE'"
   ],
   "metadata": {
    "id": "gyi5Ljv8TpjO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install portalocker>=2.0.0 # 해당 셀 실행 이후 '런타임 > 런타임 다시 시작' 후 위에서부터 다시 실행"
   ],
   "metadata": {
    "id": "fDk49qpsU_BK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard"
   ],
   "metadata": {
    "id": "HCI6OuVr7a1j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ],
   "metadata": {
    "id": "NMAuwnOwVBOu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "import modules you need\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "id": "I6fOONHU7aMR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE))\n",
    "print(\"Using torchtext version: {}\".format(torchtext.__version__))"
   ],
   "metadata": {
    "id": "yn5FYiiQVC2k"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "id": "E4E1p3w-VLTq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Load AG_NEWS dataset and set up the tokenizer and encoder pipeline.\n",
    "\n",
    "Do NOT modify.\n",
    "\"\"\"\n",
    "\n",
    "train_data, test_data = torchtext.datasets.AG_NEWS(root='./data')\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "encoder = build_vocab_from_iterator(tokens(train_data), specials=[\"<unk>\"])\n",
    "encoder.set_default_index(encoder[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: encoder(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ],
   "metadata": {
    "id": "NXSORPxqn7lU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_batch(\n",
    "    batch: List[Tuple[int, str]]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Creates a batch of encoded text, label and token length tensors.\n",
    "\n",
    "    Question (a)\n",
    "    - The input texts in the batch have different lengths.\n",
    "    - Complete your code to make them have same length using their average.\n",
    "    - This means that the length of token sequence in each batch is determined by\n",
    "      the average of token length of all sequences in each batch.\n",
    "    - Text tensors are stacked with dimension of (TOKEN_LENGTH, BATCH),\n",
    "      for easier process in RNN model.\n",
    "    - Token length tensors are used to index the last valid hidden token for classification.\n",
    "\n",
    "    Args:\n",
    "      batch: list of tuples, each containing an integer label and a text input.\n",
    "      - ex) [(3, \"Wall St. Bears...\"), (4, \"Comtes, Asteroids and ...\"), ...]\n",
    "      - number of tuples in the list is same as BATCH SIZE.\n",
    "\n",
    "    Returns:\n",
    "      text_list: batch of encoded long type text tensors with size (TOKEN_LENGTH, BATCH)\n",
    "      label_list: batch of label tensors with size (BATCH)\n",
    "      len_list: batch of token length tensors with size (BATCH)\n",
    "    \"\"\"\n",
    "\n",
    "    ##### YOUR CODE #####\n",
    "    text_list, label_list, len_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.long)\n",
    "        text_list.append(processed_text)\n",
    "        len_list.append(processed_text.size(0))\n",
    "        \n",
    "    avg_len = int(sum(len_list) / len(len_list))\n",
    "    text_list = torch.stack([F.pad(text, (0, 0, 0, avg_len - text.size(0))) for text in text_list])\n",
    "    len_list = torch.tensor(len_list, dtype=torch.long)\n",
    "    \n",
    "    text_tensor = torch.stack(text_list, dim=1)\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.long)\n",
    "    len_tensor = torch.stack(len_list, dim=0)\n",
    "    \n",
    "    assert text_list.size(1) == len(batch)\n",
    "\n",
    "    return (text_list, label_list, len_list)\n",
    "    #####################"
   ],
   "metadata": {
    "id": "jpb0L6hvmh2M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Load the data loader.\n",
    "\n",
    "Do NOT modify.\n",
    "\"\"\"\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_data)\n",
    "test_dataset = to_map_style_dataset(test_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, collate_fn=collate_batch)"
   ],
   "metadata": {
    "id": "nJXoUuq0NgiV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Print out the first batch in the train loader.\n",
    "Check if the collate function is implemented correctly.\n",
    "\n",
    "Do NOT modify.\n",
    "\"\"\"\n",
    "\n",
    "batch_x, batch_y, len_x = next(iter(train_dataloader))\n",
    "print(batch_x[:10])\n",
    "print(batch_y[:10])\n",
    "print(len_x[:10])"
   ],
   "metadata": {
    "id": "4X2_aQKhxWV3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Plot the sequence length distribution of the batches in the train dataloader.\n",
    "Make sure that all batches have difference sequence lengths.\n",
    "\n",
    "Do NOT modify.\n",
    "\"\"\"\n",
    "\n",
    "batch_len = []\n",
    "for batch_x, _, _ in train_dataloader:\n",
    "    seq_len = batch_x.size(0)\n",
    "    batch_len.append(seq_len)\n",
    "plt.hist(batch_len)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "95s-BUHc37O_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "id": "kR_cAYRUVbor"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_class: int,\n",
    "        dropout_ratio: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Define the model weight parameters and initialize the weights.\n",
    "\n",
    "        Question (b)\n",
    "        - Complete the dimension and shape of the weights and biases.\n",
    "        - Use the model parameters (vocab_size, input_size, hidden_size, num_class).\n",
    "\n",
    "        Args:\n",
    "          vocab_size: size of dictionary of vocabularies.\n",
    "          input_size: size of each embedding vector.\n",
    "          hidden_size: size of hidden dimension.\n",
    "          num_class: size of output classes.\n",
    "          dropout_ratio: probability of an element to be zeroed.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        ##### YOUR CODE #####\n",
    "        whh_size = None\n",
    "        wxh_size = None\n",
    "        why_size = None\n",
    "        bhh_size = None\n",
    "        bxh_size = None\n",
    "        bhy_size = None\n",
    "        #####################\n",
    "\n",
    "        kwargs = {'device': DEVICE, 'dtype': torch.float}\n",
    "        self.dropout = dropout_ratio\n",
    "        self.hidden = hidden_size\n",
    "        self.num_class = num_class\n",
    "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
    "        self.W_hh = nn.parameter.Parameter(torch.empty(whh_size, **kwargs))\n",
    "        self.W_xh = nn.parameter.Parameter(torch.empty(wxh_size, **kwargs))\n",
    "        self.W_hy = nn.parameter.Parameter(torch.empty(why_size, **kwargs))\n",
    "        self.b_hh = nn.parameter.Parameter(torch.empty(bhh_size, **kwargs))\n",
    "        self.b_xh = nn.parameter.Parameter(torch.empty(bxh_size, **kwargs))\n",
    "        self.b_hy = nn.parameter.Parameter(torch.empty(bhy_size, **kwargs))\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the parameters with Kaiming uniform initialization.\n",
    "\n",
    "        Do NOT modify this method.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.W_hh, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_hh)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.b_hh, -bound, bound)\n",
    "        nn.init.kaiming_uniform_(self.W_xh, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_xh)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.b_xh, -bound, bound)\n",
    "        nn.init.kaiming_uniform_(self.W_hy, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_hy)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.b_hy, -bound, bound)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, length: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Question (c)\n",
    "        - Randomly initialize h_0 with appropriate shape.\n",
    "        - Pass a sequence of tokens into the recurrent network.\n",
    "        - Implement dropout to embedded tokens with the given probability (self.dropout).\n",
    "          For example, if self.dropout is 0.3, 30% of the embedded tokens will be dropped out.\n",
    "        - We do not want to use a hidden cell of a zero-padded token for classification!\n",
    "        - Index the hidden cell of the last valid token (excluding the zero-padding)\n",
    "          based on the token length of each example in the batch.\n",
    "        - Do NOT use pre-defined PyTorch layers for this question. (e.g. nn.RNN, nn.Dropout)\n",
    "\n",
    "        Args:\n",
    "          inputs: a batch of encoded token sequences with shape (SEQ_LEN, BATCH_SIZE)\n",
    "          length: a batch of token lengths with shape (BATCH_SIZE)\n",
    "\n",
    "        Returns:\n",
    "          Softmax probabilites for each class with shape (BATCH_SIZE, NUM_CLASS)\n",
    "        \"\"\"\n",
    "\n",
    "        ##### YOUR CODE #####\n",
    "        softmax_probs = None\n",
    "\n",
    "        return softmax_probs\n",
    "        #####################\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        prediction: torch.Tensor,\n",
    "        label: torch.Tensor) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Question (d)\n",
    "\n",
    "        - Compute the cross entropy loss and the number of correct predictions\n",
    "        - Do NOT use loss function in torch.nn library (e.g. nn.CrossEntropyLoss())\n",
    "\n",
    "        Args:\n",
    "          prediction: output(softmax probabilities) from self.forward function with shape (BATCH_SIZE, NUM_CLASS)\n",
    "          label: integer labels of the batch inputs with shape (BATCH_SIZE)\n",
    "\n",
    "        Returns:\n",
    "          cross entropy loss of the batch (float tensor) and the number of correct predictions (integer)\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE #####\n",
    "        loss = None\n",
    "        correct = None\n",
    "\n",
    "        return (loss, correct)\n",
    "        #####################"
   ],
   "metadata": {
    "id": "9CMsOucsVbVE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Modules"
   ],
   "metadata": {
    "id": "zIdypCAFscfV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ScheduledOptim():\n",
    "    \"\"\"\n",
    "    Learning rate scheduler.\n",
    "\n",
    "    Do NOT modify.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, n_warmup_steps, decay_rate):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.decay = decay_rate\n",
    "        self.n_steps = 0\n",
    "        self.initial_lr = optimizer.param_groups[0]['lr']\n",
    "        self.current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.current_lr\n",
    "\n",
    "    def update(self):\n",
    "        if self.n_steps < self.n_warmup_steps:\n",
    "            lr = self.n_steps / self.n_warmup_steps * self.initial_lr\n",
    "        elif self.n_steps == self.n_warmup_steps:\n",
    "            lr = self.initial_lr\n",
    "        else:\n",
    "            lr = self.current_lr * self.decay\n",
    "\n",
    "        self.current_lr = lr\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        self.n_steps += 1"
   ],
   "metadata": {
    "id": "o9lUSQJjsfrQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {
    "id": "a1qXY-CDNIia"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Functions for training and evaluating the model.\n",
    "\n",
    "Question (e)\n",
    "- Compared to practice 3 (RNN Text Classification) covered in the lab session 3, there has been minor\n",
    "  modification with scheduler and loss computation. Check what should have been changed, and complete\n",
    "  the train and evaluate function that works for the current training pipeline\n",
    "- Use the methods of the ScheduledOptim class above to perform necessary operations on the optimizer.\n",
    "- Do NOT change the arguments given to the train, evaluate functions.\n",
    "\"\"\"\n",
    "\n",
    "def train(model, train_loader, scheduler):\n",
    "    ##### YOUR CODE #####\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    tqdm_bar = tqdm(train_loader)\n",
    "\n",
    "    for text, label, length in tqdm_bar:\n",
    "        text = text.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        length = length.to(DEVICE)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = 100. * correct / len(train_loader.dataset)\n",
    "\n",
    "    return train_loss, train_acc\n",
    "    #####################\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    ##### YOUR CODE #####\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    tqdm_bar = tqdm(test_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, label, length in tqdm_bar:\n",
    "            text = text.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            length = length.to(DEVICE)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, test_acc\n",
    "    #####################"
   ],
   "metadata": {
    "id": "98JpBhigcpJM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Question (f)\n",
    "- Train your RNN model and obtain the test accuracy of 70%.\n",
    "- Select the input size, hidden size of your choice\n",
    "- Try various optimizer type, learning rate and scheduler options for the best performance.\n",
    "- Visualize your experiments with Tensorboard.\n",
    "- Your TensorBoard results should include Train/Validation Loss and Accuracy.\n",
    "\"\"\"\n",
    "\n",
    "##### YOUR CODE #####\n",
    "writer = SummaryWriter(log_dir=\"./logs\")\n",
    "EPOCHS = 0\n",
    "BATCH_SIZE = 0\n",
    "vocab_size = 0\n",
    "input_size = 0\n",
    "hidden_size = 0\n",
    "num_class = 0\n",
    "dropout_ratio = 0\n",
    "learning_rate = 0\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "scheduler = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss_train, accu_train = train(model, train_dataloader, scheduler)\n",
    "    loss_val, accu_val = evaluate(model, valid_dataloader)\n",
    "    lr = scheduler.get_lr()\n",
    "    print('-' * 83)\n",
    "    print('| end of epoch {:2d} | lr: {:5.4f} | train loss: {:8.3f} | train accuracy: {:8.3f} | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch, lr, loss_train, accu_train, accu_val))\n",
    "    print('-' * 83)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "#####################"
   ],
   "metadata": {
    "id": "EA061dLNNKg2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# (f) Visualize the experimental logs with TensorBoard\n",
    "# Submission of the visualization result is not required.\n",
    "%tensorboard --logdir ./logs"
   ],
   "metadata": {
    "id": "ySBOjNPCz5S2"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
